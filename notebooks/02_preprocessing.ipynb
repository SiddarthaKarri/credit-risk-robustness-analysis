{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02. Feature Engineering & Preprocessing\n",
                "\n",
                "## Goal\n",
                "- Load the split data (from Google Drive).\n",
                "- Handle missing values (imputation).\n",
                "- Feature Engineering (cleaning, encoding Cat features).\n",
                "- Normalize Numeric features.\n",
                "- Save ready-to-train datasets back to Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mounted at /content/drive\n",
                        "Data Directory: /content/drive/MyDrive/credit_risk_project/data/processed\n"
                    ]
                }
            ],
            "source": [
                "# --- DRIVE SETUP ---\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    PROJECT_DIR = '/content/drive/MyDrive/credit_risk_project'\n",
                "    DATA_DIR = os.path.join(PROJECT_DIR, 'data/processed')\n",
                "except ImportError:\n",
                "    # Local fallback\n",
                "    PROJECT_DIR = '..'\n",
                "    DATA_DIR = '../data/processed'\n",
                "\n",
                "print(f\"Data Directory: {DATA_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded data successfully.\n"
                    ]
                }
            ],
            "source": [
                "# Load Processed Data from Step 01\n",
                "try:\n",
                "    df_train = pd.read_pickle(os.path.join(DATA_DIR, 'train_2014_2016.pkl'))\n",
                "    df_ood = pd.read_pickle(os.path.join(DATA_DIR, 'ood_2018_2019.pkl'))\n",
                "    print(\"Loaded data successfully.\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Error: Pickle files not found. Did you run 01_eda.ipynb?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train: (891754, 33)\n",
                        "OOD: (56318, 33)\n"
                    ]
                }
            ],
            "source": [
                "# Shape check\n",
                "print(f\"Train: {df_train.shape}\")\n",
                "print(f\"OOD: {df_ood.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top missing features:\n",
                        "emp_title     0.060440\n",
                        "emp_length    0.058864\n",
                        "title         0.018661\n",
                        "revol_util    0.000512\n",
                        "dti           0.000045\n",
                        "dtype: float64\n"
                    ]
                }
            ],
            "source": [
                "# --- 1. HANDLING MISSING VALUES ---\n",
                "missing_train = df_train.isnull().mean()\n",
                "missing_cols = missing_train[missing_train > 0].sort_values(ascending=False)\n",
                "print(\"Top missing features:\")\n",
                "print(missing_cols.head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define column types for processing\n",
                "# Note: ensuring we only select columns that actually exist in dataframe\n",
                "numeric_cols = [\n",
                "    'loan_amnt', 'term', 'int_rate', 'installment', 'annual_inc', 'dti', \n",
                "    'fico_range_low', 'fico_range_high', 'open_acc', 'pub_rec', \n",
                "    'revol_bal', 'revol_util', 'total_acc', 'mort_acc', 'pub_rec_bankruptcies'\n",
                "]\n",
                "# Filter numeric cols to available ones\n",
                "numeric_cols = [c for c in numeric_cols if c in df_train.columns]\n",
                "\n",
                "categorical_cols = [\n",
                "    'grade', 'sub_grade', 'home_ownership', 'verification_status', 'purpose', \n",
                "    'application_type', 'initial_list_status'\n",
                "]\n",
                "# Filter cat cols\n",
                "categorical_cols = [c for c in categorical_cols if c in df_train.columns]\n",
                "\n",
                "# Clean 'term' column (numeric conversion)\n",
                "def clean_term(df):\n",
                "    if df['term'].dtype == 'O':\n",
                "        return df['term'].str.extract('(\\\\d+)').astype(float)\n",
                "    return df['term']\n",
                "\n",
                "df_train['term'] = clean_term(df_train)\n",
                "df_ood['term'] = clean_term(df_ood)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Imputation Completed.\n"
                    ]
                }
            ],
            "source": [
                "# Imputation\n",
                "imputer_num = SimpleImputer(strategy='median')\n",
                "df_train[numeric_cols] = imputer_num.fit_transform(df_train[numeric_cols])\n",
                "df_ood[numeric_cols] = imputer_num.transform(df_ood[numeric_cols])\n",
                "\n",
                "imputer_cat = SimpleImputer(strategy='constant', fill_value='Missing')\n",
                "df_train[categorical_cols] = imputer_cat.fit_transform(df_train[categorical_cols])\n",
                "df_ood[categorical_cols] = imputer_cat.transform(df_ood[categorical_cols])\n",
                "\n",
                "print(\"Imputation Completed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Encoded Shapes - Train: (891754, 38), OOD: (56318, 38)\n"
                    ]
                }
            ],
            "source": [
                "# --- 2. ENCODING CATEGORICAL FEATURES ---\n",
                "# Ordinal Encoding for Grade\n",
                "grade_map = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7}\n",
                "if 'grade' in df_train.columns:\n",
                "    df_train['grade'] = df_train['grade'].map(grade_map)\n",
                "    df_ood['grade'] = df_ood['grade'].map(grade_map)\n",
                "\n",
                "# One-Hot Encoding for others\n",
                "cat_encode_list = ['home_ownership', 'verification_status', 'purpose', 'application_type', 'initial_list_status']\n",
                "cat_encode_list = [c for c in cat_encode_list if c in df_train.columns]\n",
                "\n",
                "# Using int dtype for cleaner output\n",
                "df_train = pd.get_dummies(df_train, columns=cat_encode_list, drop_first=True, dtype=int)\n",
                "df_ood = pd.get_dummies(df_ood, columns=cat_encode_list, drop_first=True, dtype=int)\n",
                "\n",
                "# Align columns\n",
                "df_train, df_ood = df_train.align(df_ood, join='left', axis=1, fill_value=0)\n",
                "\n",
                "# --- CRITICAL STEP: Drop Remaining Object Columns ---\n",
                "# Drop string columns that were not encoded (e.g. sub_grade, emp_title, zip_code, dates etc.)\n",
                "# This prevents errors in model training.\n",
                "df_train = df_train.select_dtypes(include=['number', 'bool'])\n",
                "df_ood = df_ood.select_dtypes(include=['number', 'bool'])\n",
                "\n",
                "print(f\"Encoded Shapes - Train: {df_train.shape}, OOD: {df_ood.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Normalization Completed.\n"
                    ]
                }
            ],
            "source": [
                "# --- 3. NORMALIZATION ---\n",
                "scaler = StandardScaler()\n",
                "# Select cols to scale: numeric + grade (if numeric)\n",
                "scale_cols = numeric_cols + (['grade'] if 'grade' in df_train.columns else [])\n",
                "\n",
                "# Only scale cols that are actually in the df\n",
                "scale_cols = [c for c in scale_cols if c in df_train.columns]\n",
                "\n",
                "df_train[scale_cols] = scaler.fit_transform(df_train[scale_cols])\n",
                "df_ood[scale_cols] = scaler.transform(df_ood[scale_cols])\n",
                "\n",
                "print(\"Normalization Completed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved final datasets to /content/drive/MyDrive/credit_risk_project/data/processed\n",
                        "IMPORTANT: Now you must RESTART RUNTIME before running 03_models.ipynb\n"
                    ]
                }
            ],
            "source": [
                "# Save Final Ready-to-Model datasets to DRIVE\n",
                "df_train.to_pickle(os.path.join(DATA_DIR, 'train_final.pkl'))\n",
                "df_ood.to_pickle(os.path.join(DATA_DIR, 'ood_final.pkl'))\n",
                "\n",
                "print(f\"Saved final datasets to {DATA_DIR}\")\n",
                "print(\"IMPORTANT: Now you must RESTART RUNTIME before running 03_models.ipynb\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
